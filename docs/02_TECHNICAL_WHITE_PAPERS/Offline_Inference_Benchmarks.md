Offline Inference Benchmarks
Engine: Ollama / Llama-3-8B-Instruct (4-bit Quantized).
Validation Latency: < 50ms (Deterministic Pydantic Shield).
Inference Latency: ~2-5 tokens/sec (Hardware dependent; optimized for Apple M-Series or high-RAM Windows laptops).
